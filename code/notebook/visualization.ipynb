{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots grads change and test loss change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# heatmap visualization\n",
    "import sys\n",
    "sys.path.insert(0,'/Users/zhengxuanw/Dropbox/ABSA-transformer/code/')\n",
    "sys.path.insert(0,'/Users/zhengxuanw/Dropbox/ABSA-transformer/code/model/')\n",
    "from model.ContextBERT import *\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from model.BiLSTM import *\n",
    "from model.BERT import *\n",
    "from model.BERTSimple import *\n",
    "from model.ContextBERT import *\n",
    "from model.HeadwiseContextBERT import *\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import (Sentihood_NLI_M_Processor,\n",
    "                            Semeval_NLI_M_Processor)\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from evaluation import *\n",
    "\n",
    "from util.processor import *\n",
    "from run_classifier_TABSA import *\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init grad\n",
    "init_grad = pickle.load(open(\"grads_out_init.p\", \"rb\"))\n",
    "\n",
    "def GradDiff(grad1, grad2):\n",
    "    batch_count = len(grad1)\n",
    "    dist = 0.0\n",
    "    cos = nn.CosineSimilarity(dim=-1, eps=1e-12)\n",
    "    sample_count = 0 \n",
    "    for i in range(0, batch_count):\n",
    "        sample_count += grad1[i].shape[0]\n",
    "        dist += cos(grad1[i], grad2[i]).sum()\n",
    "    return dist / sample_count\n",
    "\n",
    "time_steps = 30\n",
    "grads_list = []\n",
    "for i in range(time_steps):\n",
    "    file_in = \"grads_out_\" + str(i+1) + \".p\"\n",
    "    curr_grad = pickle.load(open(file_in, \"rb\"))\n",
    "    grads_list.append(GradDiff(curr_grad, init_grad).tolist())\n",
    "\n",
    "# quasi semeval\n",
    "grads = [1, 0.7454711198806763, 0.771233081817627, 0.7722960114479065, 0.7754009366035461, 0.7392681837081909, 0.7296900153160095, 0.7294723391532898, 0.7259636521339417, 0.7202203869819641, 0.7011971473693848, 0.5905196070671082, 0.5566345453262329, 0.46856555342674255, 0.42534950375556946, 0.5280004143714905, 0.5969454646110535, 0.5390850901603699, 0.4423533082008362, 0.35109469294548035, 0.4064408838748932, 0.34320950508117676, 0.3228585124015808, 0.28380513191223145, 0.27977651357650757, 0.4656696319580078, 0.30781835317611694, 0.32146260142326355, 0.4753910303115845, 0.41164177656173706, 0.28187230229377747]\n",
    "grads = grads[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = [0, 0, 0.02, 0.06, 0.10844, \n",
    "             0.0324, 0.24655, 0.3123, 0.779249, 0.816008,\n",
    "             0.8864097363083164, 0.8972431077694235, 0.8982278481012658,\n",
    "             0.8947630922693267, 0.8964143426294822, 0.898580121703854,\n",
    "             0.8984220907297831, 0.896551724137931, 0.9009649568308786,\n",
    "             0.887634684453566, 0.9015188633023027, 0.887202832574608, \n",
    "             0.8989547038327527, 0.887218045112782, 0.8923235445646575]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "g, = ax.plot(grads, linestyle='-', marker='s', color='b', markerfacecolor='none')\n",
    "f, = ax.plot(f1_scores, linestyle='-', marker='^', color='r', markerfacecolor='none')\n",
    "plt.legend([g, f], ['Gradient', 'Acurracy'], fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', linestyle='-.')\n",
    "plt.xlim(0, 24)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=30)\n",
    "plt.xticks([0, 5, 10, 15, 20, 24], [\"1\", \"5\", \"10\", \"15\", \"20\", \"25\"], fontsize=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-quasi semeval\n",
    "f1_scores = [0, 0, 0.023054755043227668, 0.038058991436726926, 0.1288076588337685,\n",
    "             0.032411820781696854, 0.11713286713286714, 0.1232638888888889, 0.22222222222222224,\n",
    "             0.22915042868277471, 0.33781965006729475, 0.21283509341998375, 0.25473843821076575, \n",
    "             0.24653312788906007, 0.25636942675159236, 0.4005847953216374, 0.7964514539181863, \n",
    "             0.7791780821917808, 0.8587229763700353, 0.8710801393728222, 0.8763468445356594, \n",
    "             0.8802005012531329, 0.8944472236118058, 0.9077050830397585, 0.9178833584715937]\n",
    "grads = [1, 0.7509065866470337, 0.768349289894104, 0.7725350856781006, 0.7672097086906433, 0.7408936619758606, 0.7251231670379639, 0.7309643626213074, 0.7327697277069092, 0.722244143486023, 0.7165133953094482, 0.7046288251876831, 0.7111701369285583, 0.7089152336120605, 0.6848648190498352, 0.6731955409049988, 0.655868649482727, 0.6494279503822327, 0.5999982953071594, 0.6499956250190735, 0.5288066864013672, 0.4675033390522003, 0.508086621761322, 0.5115203857421875, 0.39776304364204407, 0.37396442890167236, 0.4799198806285858, 0.3657478094100952, 0.3111937642097473, 0.38044482469558716, 0.3453109860420227]\n",
    "grads = grads[:25]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "g, = ax.plot(grads, linestyle='-', marker='s', color='b', markerfacecolor='none')\n",
    "f, = ax.plot(f1_scores, linestyle='-', marker='^', color='r', markerfacecolor='none')\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', linestyle='-.')\n",
    "plt.xlim(0, 24)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=30)\n",
    "plt.xticks([0, 5, 10, 15, 20, 24], [\"1\", \"5\", \"10\", \"15\", \"20\", \"25\"], fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quasi-sentihood\n",
    "f1_scores = [0.45471582266365185, 0.7887223811939732, 0.8013508765897253, 0.8182574715096568,\n",
    "             0.8218113806155452, 0.8076130582054173, 0.8111011294004168, 0.8210133092648388, \n",
    "            0.8102800153543908, 0.8082521995934046, 0.8143646397042046, 0.8059704504217384,\n",
    "            0.8190996191958383, 0.8652401772363258, 0.9209375625228751, 0.9140689398660004, \n",
    "            0.9247600064538062, 0.9376626454584225, 0.9073721034398871, 0.9234180270394464, \n",
    "            0.9221389351646975, 0.9272878330873275, 0.9372103640775333, 0.9258522953640869, \n",
    "            0.9452853977113524, 0.9552853977113524]\n",
    "grads = [1, 0.6710836291313171, 0.664985716342926, 0.6467165350914001, 0.6104057431221008, 0.5920002460479736, 0.5893218517303467, 0.5591245293617249, 0.5217680335044861, 0.5314010381698608, 0.5395297408103943, 0.5155467391014099, 0.5178194046020508, 0.5221627950668335, 0.5345546007156372, 0.4713890552520752, 0.477617084980011, 0.4602060914039612, 0.4370739758014679, 0.4571017026901245, 0.36851853132247925, 0.39174631237983704, 0.41272562742233276, 0.3163086771965027, 0.34830227494239807, 0.4129435122013092, 0.3338128328323364, 0.33092233538627625, 0.25370097160339355, 0.3036431074142456, 0.3181217908859253]\n",
    "grads = grads[:25]\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "g, = ax.plot(grads, linestyle='-', marker='s', color='b', markerfacecolor='none')\n",
    "f, = ax.plot(f1_scores, linestyle='-', marker='^', color='r', markerfacecolor='none')\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', linestyle='-.')\n",
    "plt.xlim(0, 24)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=30)\n",
    "plt.xticks([0, 5, 10, 15, 20, 24], [\"1\", \"5\", \"10\", \"15\", \"20\", \"25\"], fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-quasi sentihood\n",
    "\n",
    "f1_scores = [0.45471582266365185, 0.7985617369217178, 0.795571233873186, 0.8102503020224604,\n",
    "             0.815624520367763, 0.6204055080409367, 0.8402420504584572, 0.9180681090948207, \n",
    "             0.9301930621985279, 0.9112134373710598, 0.9383638585217782, 0.9456719251140596, \n",
    "             0.9500713691786066, 0.9523813647572223, 0.9488866028626513, 0.9661040398921299,\n",
    "             0.9492457781154886, 0.9409183620687862, 0.9395640477867347, 0.9336767506962838,\n",
    "             0.9435195364711321, 0.9122093067859192, 0.9377301545499019, 0.9430406470274768, \n",
    "             0.9428719557916341, 0.9528719557916341]\n",
    "grads = [1, 0.7110836291313171, 0.724985716342926, 0.7067165350914001, 0.7104057431221008, 0.6920002460479736, \n",
    "         0.6793218517303467, 0.6291245293617249, 0.6517680335044861, 0.6414010381698608, 0.6395297408103943, \n",
    "         0.6355467391014099, 0.6478194046020508, 0.6121627950668335, 0.5845546007156372, 0.5713890552520752, \n",
    "         0.527617084980011, 0.5502060914039612, 0.5170739758014679, 0.4971017026901245, 0.51851853132247925, \n",
    "         0.48174631237983704, 0.47272562742233276, 0.4163086771965027, 0.39830227494239807, 0.4129435122013092, \n",
    "         0.3338128328323364, 0.33092233538627625, 0.25370097160339355, 0.3036431074142456, 0.3181217908859253]\n",
    "grads = grads[:25]\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "g, = ax.plot(grads, linestyle='-', marker='s', color='b', markerfacecolor='none')\n",
    "f, = ax.plot(f1_scores, linestyle='-', marker='^', color='r', markerfacecolor='none')\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', linestyle='-.')\n",
    "plt.xlim(0, 24)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=30)\n",
    "plt.xticks([0, 5, 10, 15, 20, 24], [\"1\", \"5\", \"10\", \"15\", \"20\", \"25\"], fontsize=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze quasi-attention weights and gates\n",
    "memo_bundle = torch.load(\"./memo_bundle.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo_bundle = memo_bundle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_probs\n",
    "# quasi_attention_scores\n",
    "# lambda_context\n",
    "# combined_attention_probs\n",
    "\n",
    "layer_index = 0\n",
    "for layer_memo in memo_bundle:\n",
    "    print(\"layer=\"+str(layer_index+1))\n",
    "#     lambda_context = layer_memo[\"lambda_q\"][0]\n",
    "#     print(lambda_context)\n",
    "#     break\n",
    "    lambda_context_avg = lambda_context.mean().tolist()\n",
    "    print(lambda_context_avg)\n",
    "    layer_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo_bundle[3][\"lambda_k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelOptimizerTokenizer(model_type, vocab_file, embed_file=None, \n",
    "                               bert_config_file=None, init_checkpoint=None,\n",
    "                               label_list=None,\n",
    "                               do_lower_case=True,\n",
    "                               num_train_steps=None,\n",
    "                               learning_rate=None,\n",
    "                               base_learning_rate=None,\n",
    "                               warmup_proportion=None):\n",
    "    if embed_file is not None:\n",
    "        # in case pretrain embeddings\n",
    "        embeddings = pickle.load(open(embed_file, 'rb'))\n",
    "        \n",
    "    # this is the model we develop\n",
    "    tokenizer = FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case, pretrain=False)\n",
    "    if bert_config_file is not None:\n",
    "        bert_config = BertConfig.from_json_file(bert_config_file)\n",
    "    else:\n",
    "        # default?\n",
    "        bert_config = BertConfig(\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=512,\n",
    "            type_vocab_size=2,\n",
    "            initializer_range=0.02\n",
    "        )\n",
    "    # overwrite the vocab size to be exact. this also save space incase\n",
    "    # vocab size is shrinked.\n",
    "    bert_config.vocab_size = len(tokenizer.vocab)\n",
    "    # model and optimizer\n",
    "    model = ContextAwareBertForSequenceClassification(\n",
    "                bert_config, len(label_list),\n",
    "                init_weight=False)\n",
    "    \n",
    "    checkpoint = torch.load(init_checkpoint, map_location='cpu')\n",
    "    adjust_checkpoint = dict()\n",
    "    for param_tensor in checkpoint.keys():\n",
    "        adjust_checkpoint[param_tensor[7:]] = checkpoint[param_tensor]\n",
    "\n",
    "    model.load_state_dict(adjust_checkpoint)\n",
    "    #######################################################################\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    #######################################################################\n",
    "    return model, optimizer, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, tokenizer = \\\n",
    "    getModelOptimizerTokenizer(\"ContextBERT\", \n",
    "                           vocab_file=\"../data/uncased_L-12_H-768_A-12/vocab.txt\",\n",
    "                           bert_config_file=\"../data/uncased_L-12_H-768_A-12/bert_config.json\",\n",
    "                           init_checkpoint=\"./sentihood_checkpoint.bin\",\n",
    "                           label_list=['None', 'Positive', 'Negative'],\n",
    "                           num_train_steps=1,\n",
    "                           learning_rate=2e-5,\n",
    "                           base_learning_rate=2e-5,\n",
    "                           warmup_proportion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 718.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_in = InputExample(guid=\"0-0\", \n",
    "                       text_a=\"Thanks LOCATION1 has highest average living costs but is safest and poshest , LOCATION2 is the roughest area but cheapest all round\", \n",
    "                       text_b='location - 1 - price', \n",
    "                       label=\"Positive\")\n",
    "\n",
    "tokens_a = tokenizer.tokenize(test_in.text_a)\n",
    "\n",
    "test_features = convert_examples_to_features(\n",
    "    [test_in], ['None', 'Positive', 'Negative'], 512,\n",
    "    tokenizer, 6,\n",
    "    True)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "all_context_ids = torch.tensor([f.context_ids for f in test_features], dtype=torch.long)\n",
    "all_context_len = torch.tensor([[f.context_len] for f in test_features], dtype=torch.long)\n",
    "target_id_convert = [[0,1,2,3], [4,5,6,7]]\n",
    "all_target_ids = torch.tensor([target_id_convert[f.target_id] for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids, all_seq_len, all_context_ids,\n",
    "                          all_context_len, all_target_ids)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 31])\n",
      "tensor([[0.0164, 0.0988, 0.8847]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "grads = None\n",
    "memo_bundle = None\n",
    "for input_ids, input_mask, segment_ids, label_ids, seq_lens, \\\n",
    "    context_ids, context_lens, all_target_ids in test_dataloader:\n",
    "    max_seq_lens = max(seq_lens)[0]\n",
    "    input_ids = input_ids[:,:max_seq_lens]\n",
    "    input_mask = input_mask[:,:max_seq_lens]\n",
    "    segment_ids = segment_ids[:,:max_seq_lens]\n",
    "    tmp_test_loss, logits, embedding_output, all_encoder_memo_bundle, _ = \\\n",
    "        model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                device=torch.device(\"cpu\"), labels=label_ids,\n",
    "                context_ids=context_ids,\n",
    "                context_lens=context_lens)\n",
    "\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    sensitivity_class = 1\n",
    "    sensitivity_grads = torch.zeros(logits.shape)\n",
    "    sensitivity_grads[:,sensitivity_class] = 1.0\n",
    "    grads_in = torch.autograd.grad(logits, embedding_output, grad_outputs=sensitivity_grads)[0]\n",
    "    grads_in_norm = torch.norm(grads_in, dim=-1) * torch.norm(grads_in, dim=-1)\n",
    "    max_grads = torch.max(grads_in_norm)\n",
    "    grads_in_norm = grads_in_norm / max_grads\n",
    "    print(grads_in_norm.shape)\n",
    "    grads = grads_in_norm.squeeze(dim=0).tolist()\n",
    "    memo_bundle = all_encoder_memo_bundle\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "tokens_a_new = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "print(len(tokens_a_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.015483628027141094),\n",
       " ('thanks', 0.07659727334976196),\n",
       " ('location', 0.02283579856157303),\n",
       " ('##1', 0.02936406061053276),\n",
       " ('has', 0.03458936884999275),\n",
       " ('highest', 0.13893410563468933),\n",
       " ('average', 0.08896587789058685),\n",
       " ('living', 0.08794022351503372),\n",
       " ('costs', 1.0),\n",
       " ('but', 0.029467273503541946),\n",
       " ('is', 0.014227394014596939),\n",
       " ('safe', 0.08752273768186569),\n",
       " ('##st', 0.03862462192773819),\n",
       " ('and', 0.016032399609684944),\n",
       " ('po', 0.15097376704216003),\n",
       " ('##sh', 0.7230157852172852),\n",
       " ('##est', 0.08469756692647934),\n",
       " (',', 0.028957849368453026),\n",
       " ('location', 0.040933165699243546),\n",
       " ('##2', 0.017027543857693672),\n",
       " ('is', 0.013930792920291424),\n",
       " ('the', 0.016729407012462616),\n",
       " ('rough', 0.12700320780277252),\n",
       " ('##est', 0.022745544090867043),\n",
       " ('area', 0.06409606337547302),\n",
       " ('but', 0.022327423095703125),\n",
       " ('cheap', 0.8606501817703247),\n",
       " ('##est', 0.2365674525499344),\n",
       " ('all', 0.06792391836643219),\n",
       " ('round', 0.04317764937877655),\n",
       " ('[SEP]', 0.01861918717622757)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens_a_new, grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_viz(token_grad):\n",
    "    scores = [tu[1] for tu in token_grad]\n",
    "    tokens = [tu[0] for tu in token_grad]\n",
    "    fig, ax = plt.subplots(figsize=(10,1))\n",
    "    ax = sns.heatmap([scores], cmap=\"Blues\", xticklabels=tokens, yticklabels=False,\n",
    "                     cbar_kws=dict(shrink=1, aspect=4, ), linewidths=0.8)\n",
    "    ax.set_xticklabels(tokens, size = 18)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    # here set the labelsize by 20\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap_viz(list(zip(tokens_a_new, grads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quasi attention distribution\n",
    "model, optimizer, tokenizer = \\\n",
    "    getModelOptimizerTokenizer(\"ContextBERT\", \n",
    "                           vocab_file=\"../data/uncased_L-12_H-768_A-12/vocab.txt\",\n",
    "                           bert_config_file=\"../data/uncased_L-12_H-768_A-12/bert_config.json\",\n",
    "                           init_checkpoint=\"./quasi/sentihood_checkpoint.bin\",\n",
    "                           label_list=['None', 'Positive', 'Negative'],\n",
    "                           num_train_steps=1,\n",
    "                           learning_rate=2e-5,\n",
    "                           base_learning_rate=2e-5,\n",
    "                           warmup_proportion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"sentihood_NLI_M\":Sentihood_NLI_M_Processor,\n",
    "    \"semeval_NLI_M\":Semeval_NLI_M_Processor\n",
    "}\n",
    "\n",
    "processor = processors[\"sentihood_NLI_M\"]()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "test_examples = processor.get_test_examples(\"../data/sentihood/bert-pair/\")\n",
    "test_features = convert_examples_to_features(\n",
    "    test_examples, label_list, 512,\n",
    "    tokenizer, 6,\n",
    "    True)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "all_context_ids = torch.tensor([f.context_ids for f in test_features], dtype=torch.long)\n",
    "all_context_len = torch.tensor([[f.context_len] for f in test_features], dtype=torch.long)\n",
    "target_id_convert = [[0,1,2,3], [4,5,6,7]]\n",
    "all_target_ids = torch.tensor([target_id_convert[f.target_id] for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids, all_seq_len, all_context_ids,\n",
    "                          all_context_len, all_target_ids)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12*[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "grads = None\n",
    "memo_bundle = None\n",
    "samples_lambda_context = []\n",
    "\n",
    "attention_probs = []\n",
    "quasi_attention_scores = []\n",
    "new_attention_probs = []\n",
    "\n",
    "count = 0\n",
    "for input_ids, input_mask, segment_ids, label_ids, seq_lens, \\\n",
    "    context_ids, context_lens, all_target_ids in test_dataloader:\n",
    "    max_seq_lens = max(seq_lens)[0]\n",
    "    input_ids = input_ids[:,:max_seq_lens]\n",
    "    input_mask = input_mask[:,:max_seq_lens]\n",
    "    segment_ids = segment_ids[:,:max_seq_lens]\n",
    "    tmp_test_loss, logits, embedding_output, all_encoder_memo_bundle, _ = \\\n",
    "        model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                device=torch.device(\"cpu\"), labels=label_ids,\n",
    "                context_ids=context_ids,\n",
    "                context_lens=context_lens)\n",
    "\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    sensitivity_class = 1\n",
    "    sensitivity_grads = torch.zeros(logits.shape)\n",
    "    sensitivity_grads[:,sensitivity_class] = 1.0\n",
    "    grads_in = torch.autograd.grad(logits, embedding_output, grad_outputs=sensitivity_grads)[0]\n",
    "    grads_in_norm = torch.norm(grads_in, dim=-1) * torch.norm(grads_in, dim=-1)\n",
    "    max_grads = torch.max(grads_in_norm)\n",
    "    grads_in_norm = grads_in_norm / max_grads\n",
    "\n",
    "    grads = grads_in_norm.squeeze(dim=0).tolist()\n",
    "    memo_bundle = all_encoder_memo_bundle\n",
    "    \n",
    "    # we consider labeled cases\n",
    "    if label_ids.tolist()[0] != 0:\n",
    "        for i in range(12):\n",
    "            lambda_context_avg = all_encoder_memo_bundle[i][\"lambda_context\"].flatten().tolist()\n",
    "            samples_lambda_context.extend(lambda_context_avg)\n",
    "            attention_probs.extend(all_encoder_memo_bundle[i][\"attention_probs\"].flatten().tolist())\n",
    "            quasi_attention_scores.extend(all_encoder_memo_bundle[i][\"quasi_attention_scores\"].flatten().tolist())\n",
    "            new_attention_probs.extend(all_encoder_memo_bundle[i][\"new_attention_probs\"].flatten().tolist())\n",
    "            \n",
    "        count += 1\n",
    "        print(\"count=\",count)\n",
    "    \n",
    "    if count == 200:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histgram(array_in,  facecolor='g', xl=-1, xh=1):\n",
    "    import matplotlib as mpl\n",
    "    mpl.style.use(\"default\")\n",
    "    font = {'family' : 'Times New Roman',\n",
    "            'size'   : 30}\n",
    "    plt.rc('font', **font)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    g = ax.hist(array_in, bins=50, facecolor=facecolor)\n",
    "    plt.grid(True)\n",
    "    plt.grid(color='black', linestyle='-.')\n",
    "    import matplotlib.ticker as mtick\n",
    "    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
    "    # ax.set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.xlim(xl, xh)\n",
    "    # plt.xticks([-1, -0.5, 0, 0.5, 1], [\"-1.0\", \"-0.50\", \"0.0\", \"0.50\", \"1.0\"], fontsize=30)\n",
    "    plt.xticks([-0.5, -0.25, 0, 0.25, 0.5], [\"-0.50\", \"-0.25\", \"0.0\", \"0.25\", \"0.5\"], fontsize=30)\n",
    "    # plt.xticks([0, 0.25, 0.5, 0.75, 1], [\"0.0\", \"0.25\", \"0.50\", \"0.75\", \"1.0\"], fontsize=20)\n",
    "    # plt.xticks([0, 0.25, 0.5, 0.75, 1], [\"0.0\", \"0.25\", \"0.50\", \"0.75\", \"1.0\"], fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histgram(samples_lambda_context, xl=-0.5, xh=0.5, facecolor='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histgram(attention_probs, facecolor=\"y\", xl=0, xh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histgram(quasi_attention_scores, facecolor=\"b\", xl=0, xh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histgram(new_attention_probs, facecolor=\"g\", xl=-1, xh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "def heatmap_viz(token_grad):\n",
    "    scores = [tu[1] for tu in token_grad]\n",
    "    tokens = [tu[0] for tu in token_grad]\n",
    "    fig, ax = plt.subplots(figsize=(10,1))\n",
    "    ax = sns.heatmap([scores], cmap=\"Blues\", xticklabels=tokens, yticklabels=False,\n",
    "                     cbar_kws=dict(shrink=1, aspect=4, ), linewidths=0.8)\n",
    "    ax.set_xticklabels(tokens, size = 20)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    # here set the labelsize by 20\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    plt.show()\n",
    "\n",
    "tokens_a_new = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "heatmap_viz(list(zip(tokens_a_new, grads)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
